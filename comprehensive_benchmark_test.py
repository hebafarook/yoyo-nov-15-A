#!/usr/bin/env python3

import requests
import sys
import json
import time
from datetime import datetime

class ComprehensiveBenchmarkTester:
    def __init__(self, base_url="https://yo-yo-coach-dash.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.access_token = None
        self.user_id = None
        self.test_username = None
        self.benchmarks = {}

    def run_test_with_auth(self, name, method, endpoint, expected_status, data=None, params=None, headers=None):
        """Run a single API test with authentication headers"""
        url = f"{self.api_url}/{endpoint}"
        if not headers:
            headers = {}
        headers['Content-Type'] = 'application/json'

        self.tests_run += 1
        print(f"\nğŸ” Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, params=params)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=headers)
            elif method == 'PUT':
                response = requests.put(url, json=data, headers=headers)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers)

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"âœ… Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    if isinstance(response_data, dict) and len(str(response_data)) < 800:
                        print(f"   Response: {response_data}")
                    elif isinstance(response_data, list):
                        print(f"   Response: List with {len(response_data)} items")
                    else:
                        print(f"   Response: Large data object received")
                except:
                    print(f"   Response: Non-JSON response")
            else:
                print(f"âŒ Failed - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error: {response.text}")

            return success, response.json() if response.content else {}

        except Exception as e:
            print(f"âŒ Failed - Error: {str(e)}")
            return False, {}

    def setup_authentication(self):
        """Setup user authentication"""
        timestamp = str(int(time.time()))
        user_data = {
            "username": f"testcoach_comprehensive_{timestamp}",
            "email": f"testcoach_comprehensive_{timestamp}@benchmark.com",
            "full_name": "Test Coach Comprehensive",
            "password": "securepassword123",
            "is_coach": True
        }
        
        success, response = self.run_test_with_auth(
            "1. User Registration",
            "POST",
            "auth/register",
            200,
            data=user_data
        )
        
        if success and 'access_token' in response:
            self.access_token = response['access_token']
            self.user_id = response['user']['id']
            self.test_username = user_data['username']
            print(f"   âœ… Authentication setup complete")
        
        return success

    def test_save_first_assessment_benchmark(self):
        """2. Save first assessment benchmark - should auto-detect as baseline"""
        if not self.access_token:
            return False
            
        benchmark_data = {
            "user_id": self.user_id,
            "player_name": "Test Player",
            "assessment_id": "test-assessment-id",
            "age": 16,
            "position": "Midfielder",
            "sprint_30m": 4.2,
            "yo_yo_test": 2400,
            "vo2_max": 55.0,
            "vertical_jump": 50,
            "body_fat": 12.0,
            "ball_control": 4,
            "passing_accuracy": 85.0,
            "dribbling_success": 75.0,
            "shooting_accuracy": 70.0,
            "defensive_duels": 65.0,
            "game_intelligence": 4,
            "positioning": 4,
            "decision_making": 4,
            "coachability": 5,
            "mental_toughness": 4,
            "overall_score": 78.5,
            "performance_level": "Advanced",
            "benchmark_type": "regular",
            "notes": "First benchmark test"
        }
        
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "2. Save First Assessment (Auto-Baseline Detection)",
            "POST",
            "auth/save-benchmark",
            200,
            data=benchmark_data,
            headers=headers
        )
        
        if success:
            self.benchmarks['first'] = response
            # Verify baseline detection
            assert response.get('is_baseline') == True, "First benchmark should be baseline"
            assert response.get('benchmark_type') == "baseline", "First benchmark type should be 'baseline'"
            assert response.get('improvement_from_baseline') is None, "First benchmark should not have improvement data"
            print(f"   âœ… Baseline auto-detection working correctly")
        
        return success

    def test_save_second_assessment_benchmark(self):
        """3. Save second assessment benchmark - should be regular type with improvement calculation"""
        if not self.access_token:
            return False
            
        benchmark_data = {
            "user_id": self.user_id,
            "player_name": "Test Player",
            "assessment_id": "test-assessment-id-2",
            "age": 16,
            "position": "Midfielder",
            "sprint_30m": 4.0,  # Improved (lower is better)
            "yo_yo_test": 2500,  # Improved
            "vo2_max": 57.0,  # Improved
            "vertical_jump": 52,  # Improved
            "body_fat": 11.5,  # Improved (lower is better)
            "ball_control": 4,
            "passing_accuracy": 87.0,  # Improved
            "dribbling_success": 78.0,  # Improved
            "shooting_accuracy": 72.0,  # Improved
            "defensive_duels": 68.0,  # Improved
            "game_intelligence": 4,
            "positioning": 4,
            "decision_making": 4,
            "coachability": 5,
            "mental_toughness": 4,
            "overall_score": 81.2,  # Improved
            "performance_level": "Advanced",
            "benchmark_type": "regular",
            "notes": "Second benchmark showing improvement"
        }
        
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "3. Save Second Assessment (Regular with Improvement)",
            "POST",
            "auth/save-benchmark",
            200,
            data=benchmark_data,
            headers=headers
        )
        
        if success:
            self.benchmarks['second'] = response
            # Verify regular benchmark with improvement
            assert response.get('is_baseline') == False, "Second benchmark should NOT be baseline"
            assert response.get('improvement_from_baseline') is not None, "Second benchmark should have improvement data"
            
            improvement = response.get('improvement_from_baseline', {})
            print(f"   âœ… Improvement calculations:")
            print(f"      Sprint 30m: {improvement.get('sprint_30m', 0)}% (lower is better)")
            print(f"      VO2 Max: {improvement.get('vo2_max', 0)}%")
            print(f"      Overall Score: {improvement.get('overall_score', 0)}%")
        
        return success

    def test_save_third_assessment_benchmark(self):
        """4. Save third assessment benchmark - verify progression tracking"""
        if not self.access_token:
            return False
            
        benchmark_data = {
            "user_id": self.user_id,
            "player_name": "Test Player",
            "assessment_id": "test-assessment-id-3",
            "age": 16,
            "position": "Midfielder",
            "sprint_30m": 3.9,  # Further improved
            "yo_yo_test": 2600,  # Further improved
            "vo2_max": 58.5,  # Further improved
            "vertical_jump": 54,  # Further improved
            "body_fat": 11.0,  # Further improved
            "ball_control": 5,  # Improved
            "passing_accuracy": 89.0,  # Further improved
            "dribbling_success": 80.0,  # Further improved
            "shooting_accuracy": 75.0,  # Further improved
            "defensive_duels": 70.0,  # Further improved
            "game_intelligence": 5,  # Improved
            "positioning": 5,  # Improved
            "decision_making": 4,
            "coachability": 5,
            "mental_toughness": 5,  # Improved
            "overall_score": 84.8,  # Further improved
            "performance_level": "Elite",
            "benchmark_type": "milestone",
            "notes": "Third benchmark showing continued progression"
        }
        
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "4. Save Third Assessment (Progression Tracking)",
            "POST",
            "auth/save-benchmark",
            200,
            data=benchmark_data,
            headers=headers
        )
        
        if success:
            self.benchmarks['third'] = response
            print(f"   âœ… Progression tracking working correctly")
        
        return success

    def test_retrieve_all_benchmarks(self):
        """5. Retrieve all benchmarks - verify sorting by benchmark_date (newest first)"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "5. Retrieve All Benchmarks (Sorted by Date)",
            "GET",
            "auth/benchmarks",
            200,
            headers=headers
        )
        
        if success:
            assert len(response) == 3, f"Expected 3 benchmarks, got {len(response)}"
            
            # Verify sorting (newest first)
            dates = [b.get('benchmark_date') for b in response]
            print(f"   âœ… Found {len(response)} benchmarks, sorted by date (newest first)")
            print(f"      Newest: {response[0].get('benchmark_type')} - {response[0].get('overall_score')}")
            print(f"      Oldest: {response[-1].get('benchmark_type')} - {response[-1].get('overall_score')}")
        
        return success

    def test_retrieve_baseline_benchmark(self):
        """6. Retrieve baseline benchmark for player"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "6. Retrieve Baseline Benchmark",
            "GET",
            "auth/benchmarks/baseline?player_name=Test Player",
            200,
            headers=headers
        )
        
        if success:
            assert response.get('is_baseline') == True, "Retrieved benchmark should be baseline"
            assert response.get('benchmark_type') == "baseline", "Benchmark type should be 'baseline'"
            print(f"   âœ… Baseline benchmark retrieved correctly")
        
        return success

    def test_retrieve_specific_benchmark(self):
        """7. Retrieve specific benchmark by ID"""
        if not self.access_token or 'second' not in self.benchmarks:
            return False
            
        benchmark_id = self.benchmarks['second']['id']
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "7. Retrieve Specific Benchmark by ID",
            "GET",
            f"auth/benchmarks/{benchmark_id}",
            200,
            headers=headers
        )
        
        if success:
            assert response.get('id') == benchmark_id, "Retrieved benchmark ID should match"
            print(f"   âœ… Specific benchmark retrieved correctly")
        
        return success

    def test_get_player_progress_analysis(self):
        """8. Get player progress analysis - verify improvement calculations and timeline"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "8. Get Player Progress Analysis",
            "GET",
            "auth/benchmarks/progress/Test Player",
            200,
            headers=headers
        )
        
        if success:
            assert response.get('total_benchmarks') == 3, f"Expected 3 benchmarks, got {response.get('total_benchmarks')}"
            assert response.get('baseline_score') == 78.5, "Baseline score should be 78.5"
            assert response.get('latest_score') == 84.8, "Latest score should be 84.8"
            
            improvement = response.get('overall_improvement')
            timeline = response.get('improvement_timeline', [])
            
            print(f"   âœ… Progress analysis working correctly:")
            print(f"      Total Benchmarks: {response.get('total_benchmarks')}")
            print(f"      Overall Improvement: {improvement}%")
            print(f"      Timeline Entries: {len(timeline)}")
        
        return success

    def test_try_delete_baseline_benchmark(self):
        """9. Try to delete baseline benchmark - should fail with 400 error"""
        if not self.access_token or 'first' not in self.benchmarks:
            return False
            
        baseline_id = self.benchmarks['first']['id']
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "9. Try to Delete Baseline Benchmark (Should Fail)",
            "DELETE",
            f"auth/benchmarks/{baseline_id}",
            400,
            headers=headers
        )
        
        if success:
            assert 'Cannot delete baseline benchmark' in response.get('detail', ''), "Should prevent baseline deletion"
            print(f"   âœ… Baseline protection working correctly")
        
        return success

    def test_delete_regular_benchmark(self):
        """10. Delete regular benchmark - should succeed"""
        if not self.access_token or 'third' not in self.benchmarks:
            return False
            
        regular_id = self.benchmarks['third']['id']
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "10. Delete Regular Benchmark (Should Succeed)",
            "DELETE",
            f"auth/benchmarks/{regular_id}",
            200,
            headers=headers
        )
        
        if success:
            print(f"   âœ… Regular benchmark deletion working correctly")
        
        return success

    def test_verify_userprofile_updated(self):
        """11. Verify UserProfile updated with benchmarks array"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "11. Verify UserProfile Updated",
            "GET",
            "auth/profile",
            200,
            headers=headers
        )
        
        if success:
            profile = response.get('profile', {})
            players_managed = profile.get('players_managed', [])
            
            assert 'Test Player' in players_managed, "Player should be in managed players list"
            print(f"   âœ… UserProfile updated correctly:")
            print(f"      Players Managed: {players_managed}")
        
        return success

    def test_error_handling(self):
        """12. Test with invalid user_id, benchmark_id - verify proper error handling"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        # Test invalid benchmark ID
        success1, _ = self.run_test_with_auth(
            "12a. Invalid Benchmark ID",
            "GET",
            "auth/benchmarks/invalid-benchmark-id",
            404,
            headers=headers
        )
        
        # Test non-existent player
        success2, _ = self.run_test_with_auth(
            "12b. Non-existent Player Progress",
            "GET",
            "auth/benchmarks/progress/NonExistentPlayer",
            404,
            headers=headers
        )
        
        print(f"   âœ… Error handling working correctly")
        return success1 and success2

    def test_filtering_by_player_name(self):
        """13. Test filtering benchmarks by player_name"""
        if not self.access_token:
            return False
            
        headers = {'Authorization': f'Bearer {self.access_token}'}
        
        success, response = self.run_test_with_auth(
            "13. Filter Benchmarks by Player Name",
            "GET",
            "auth/benchmarks?player_name=Test Player",
            200,
            headers=headers
        )
        
        if success:
            # Should have 2 benchmarks left (after deleting the third one)
            assert len(response) == 2, f"Expected 2 benchmarks after deletion, got {len(response)}"
            
            # Verify all returned benchmarks are for the correct player
            for benchmark in response:
                assert benchmark.get('player_name') == 'Test Player', "All benchmarks should be for 'Test Player'"
            
            print(f"   âœ… Player name filtering working correctly")
        
        return success

def main():
    print("ğŸ”¥ COMPREHENSIVE ASSESSMENT BENCHMARK SYSTEM TEST")
    print("Testing all scenarios from the review request")
    print("=" * 80)
    
    tester = ComprehensiveBenchmarkTester()
    
    # Test sequence following the exact scenarios from the review request
    test_results = []
    
    # 1. User authentication flow
    test_results.append(tester.setup_authentication())
    
    # 2. Save first assessment benchmark - should auto-detect as baseline
    test_results.append(tester.test_save_first_assessment_benchmark())
    
    # 3. Save second assessment benchmark - should be regular type with improvement
    test_results.append(tester.test_save_second_assessment_benchmark())
    
    # 4. Save third assessment benchmark - verify progression tracking
    test_results.append(tester.test_save_third_assessment_benchmark())
    
    # 5. Retrieve all benchmarks - verify sorting by benchmark_date (newest first)
    test_results.append(tester.test_retrieve_all_benchmarks())
    
    # 6. Retrieve baseline benchmark for player
    test_results.append(tester.test_retrieve_baseline_benchmark())
    
    # 7. Retrieve specific benchmark by ID
    test_results.append(tester.test_retrieve_specific_benchmark())
    
    # 8. Get player progress analysis - verify improvement calculations and timeline
    test_results.append(tester.test_get_player_progress_analysis())
    
    # 9. Try to delete baseline benchmark - should fail with 400 error
    test_results.append(tester.test_try_delete_baseline_benchmark())
    
    # 10. Delete regular benchmark - should succeed
    test_results.append(tester.test_delete_regular_benchmark())
    
    # 11. Verify UserProfile updated with benchmarks array
    test_results.append(tester.test_verify_userprofile_updated())
    
    # 12. Test with invalid user_id, benchmark_id - verify proper error handling
    test_results.append(tester.test_error_handling())
    
    # 13. Test filtering benchmarks by player_name
    test_results.append(tester.test_filtering_by_player_name())
    
    # Print final results
    print("\n" + "=" * 80)
    print("ğŸ“Š COMPREHENSIVE TEST RESULTS")
    print("=" * 80)
    print(f"Tests Run: {tester.tests_run}")
    print(f"Tests Passed: {tester.tests_passed}")
    print(f"Tests Failed: {tester.tests_run - tester.tests_passed}")
    print(f"Success Rate: {(tester.tests_passed / tester.tests_run * 100):.1f}%")
    
    if tester.tests_passed == tester.tests_run:
        print("ğŸ‰ ALL ASSESSMENT BENCHMARK SYSTEM TESTS PASSED!")
        print("\nâœ… VERIFIED FEATURES:")
        print("   â€¢ Baseline auto-detection logic")
        print("   â€¢ Improvement calculation accuracy")
        print("   â€¢ Proper MongoDB storage and retrieval")
        print("   â€¢ Authentication/authorization")
        print("   â€¢ Edge cases and error handling")
        print("   â€¢ UserProfile integration")
        print("   â€¢ Baseline protection (cannot delete)")
        print("   â€¢ Progress analysis and timeline tracking")
        return 0
    else:
        print("âš ï¸  Some tests failed. Check the details above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())